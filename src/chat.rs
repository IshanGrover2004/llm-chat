use std::{convert::Infallible, io::Write, path::PathBuf};

use serde::{Deserialize, Serialize};

#[derive(thiserror::Error, Debug)]
enum InferenceError {
    #[error("Failed to load the model: {0}")]
    UnableToLoadModel(String),

    #[error("Failed to perform inference: {0}")]
    UnableToCreateResponse(String),
}

/// Represents a user prompt.
#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct Prompt {
    prompt: Option<String>,
}

/// Represents a response generated by the model.
#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct Response {
    response: String,
}

impl Response {
    /// Creates a new Response instance with the given response string.
    pub fn new(response: String) -> Self {
        Self { response }
    }

    /// Gets the response string.
    pub fn response(&self) -> String {
        self.response.to_owned()
    }
}

impl Prompt {
    /// Creates a new Prompt instance.
    pub fn _new(prompt: Option<String>) -> Self {
        Self { prompt }
    }

    /// Gets the prompt string.
    pub fn prompt(&self) -> Option<String> {
        self.prompt.to_owned()
    }

    /// Performs inference based on the prompt
    pub fn infer(&self) -> anyhow::Result<Response> {
        let path: &str = "/home/tan/Documents/Projects/llm_task/my_llm/open_llama_3b-f16.bin";

        // What model to use
        let model_architecture = llm::ModelArchitecture::Llama;
        // Path of model binary file
        let model_path = PathBuf::from(path);
        // Source of tokenizer
        let tokenizer_source = llm::TokenizerSource::Embedded;
        // Prompt to ask
        let prompt = self.prompt.as_deref().unwrap();
        // Time at which program is executed
        let now_time = std::time::Instant::now();

        // Attempt to load the model
        let model = llm::load_dynamic(
            Some(model_architecture),
            &model_path,
            tokenizer_source,
            Default::default(),
            llm::load_progress_callback_stdout,
        )
        .map_err(|e| InferenceError::UnableToLoadModel(e.to_string()))?;

        println!(
            "Model fully loaded! Elapsed: {}ms",
            now_time.elapsed().as_millis()
        );

        // Starting session
        let mut session = model.start_session(Default::default());

        // LLM response
        let mut result = String::new();

        session
            .infer::<Infallible>(
                // Model to use
                model.as_ref(),
                // Random range
                &mut rand::thread_rng(),
                // Request for inference
                &llm::InferenceRequest {
                    prompt: (prompt).into(),
                    parameters: &llm::InferenceParameters::default(),
                    play_back_previous_tokens: false,
                    maximum_token_count: Some(100),
                },
                // Output request
                &mut Default::default(),
                // Inference response
                |r| match r {
                    llm::InferenceResponse::PromptToken(t)
                    | llm::InferenceResponse::InferredToken(t) => {
                        print!("{t}");
                        result.push_str(&t);

                        std::io::stdout().flush().unwrap();

                        Ok(llm::InferenceFeedback::Continue)
                    }
                    _ => Ok(llm::InferenceFeedback::Continue),
                },
            )
            .map_err(|e| InferenceError::UnableToCreateResponse(e.to_string()))?;

        Ok(Response::new(result.to_string()))
    }
}
